{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7466ea6f-9404-4e3d-97f8-70550611a761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: 694d524a-f6c1-4c83-843f-20cf0d9296e1)')' thrown while requesting HEAD https://huggingface.co/datasets/glue/resolve/main/README.md\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: 590ecdcd-6f21-4e3f-834c-b72adf9ac99a)')' thrown while requesting HEAD https://huggingface.co/datasets/glue/resolve/main/README.md\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: d1166312-8bcf-4dae-b4ae-95d4967cd891)')' thrown while requesting HEAD https://huggingface.co/datasets/glue/resolve/main/README.md\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: cd7a440b-b52e-46d4-be82-ca4fc878d88b)')' thrown while requesting HEAD https://huggingface.co/datasets/glue/resolve/main/README.md\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: 0046d5c7-e143-4ab8-b54a-d8b36b36e41c)')' thrown while requesting HEAD https://huggingface.co/datasets/glue/resolve/main/README.md\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: 42e984fe-ff7d-4dd6-95af-c520cec16e12)')' thrown while requesting HEAD https://huggingface.co/datasets/glue/resolve/main/README.md\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: 4ee60029-70d2-4b45-8fcb-970aed9e4650)')' thrown while requesting HEAD https://huggingface.co/datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/glue.py\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: 977f8f82-78c0-45a4-befa-e3db82ffce4b)')' thrown while requesting HEAD https://huggingface.co/datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/glue.py\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: 0b1a4522-a000-41c2-93ac-28dfe2dd3fc0)')' thrown while requesting HEAD https://huggingface.co/datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/glue.py\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/glue.py (Caused by SSLError(SSLCertVerificationError(1, \"[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for \\'huggingface.co\\'. (_ssl.c:1028)\")))'), '(Request ID: 9713de20-aae6-4454-b8e8-50ffc88c6c77)')' thrown while requesting HEAD https://huggingface.co/datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/glue.py\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/glue.py (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000002A8100C9A90>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 4d4038dc-9510-48da-a68f-f4f791db8476)')' thrown while requesting HEAD https://huggingface.co/datasets/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/glue.py\n",
      "Retrying in 8s [Retry 5/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集信息:\n",
      "类型: <class 'datasets.arrow_dataset.Dataset'>\n",
      "样本数: 20\n",
      "特征: {'sentence': Value('string'), 'label': ClassLabel(names=['negative', 'positive']), 'idx': Value('int32')}\n",
      "\n",
      "前3个样本:\n",
      "样本 1:\n",
      "  文本: uneasy mishmash of styles and genres .\n",
      "  标签: -1\n",
      "\n",
      "样本 2:\n",
      "  文本: this film 's relationship to actual tension is the same as what christmas-tree flocking in a spray can is to actual snow : a poor -- if durable -- imitation .\n",
      "  标签: -1\n",
      "\n",
      "样本 3:\n",
      "  文本: by the end of no such thing the audience , like beatrice , has a watchful affection for the monster .\n",
      "  标签: -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 加载SST-2数据集的前20个测试样本\n",
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"test[:20]\")\n",
    "\n",
    "print(\"数据集信息:\")\n",
    "print(f\"类型: {type(dataset)}\")\n",
    "print(f\"样本数: {len(dataset)}\")\n",
    "print(f\"特征: {dataset.features}\")\n",
    "\n",
    "print(\"\\n前3个样本:\")\n",
    "for i in range(3):\n",
    "    print(f\"样本 {i+1}:\")\n",
    "    print(f\"  文本: {dataset[i]['sentence']}\")\n",
    "    print(f\"  标签: {dataset[i]['label']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "00b485a2-ce3e-4cf9-b90f-af66a8da6275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始情感分析...\n",
      "\n",
      "结果统计:\n",
      "总样本数: 20\n",
      "有效预测数: 18\n",
      "有效预测准确率: 22.22%\n",
      "无效预测数: 2\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"test[:20]\")\n",
    "\n",
    "def query_ollama(prompt, model=\"llama2:7b\"):\n",
    "    \"\"\"调用Ollama模型\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.1,  # 降低随机性\n",
    "            \"num_predict\": 50    # 限制输出长度\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=data, timeout=60)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"response\"]\n",
    "        else:\n",
    "            return f\"错误: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"请求失败: {str(e)}\"\n",
    "\n",
    "# 改进的提示词\n",
    "improved_prompt = \"\"\"判断以下电影评论的情感，只返回一个数字：\n",
    "- 正面情感返回 1\n",
    "- 负面情感返回 -1\n",
    "\n",
    "评论：\"{0}\"\n",
    "\n",
    "情感：\"\"\"\n",
    "\n",
    "def robust_sentiment_parser(response):\n",
    "    \"\"\"鲁棒的情感解析器\"\"\"\n",
    "    if not response or len(response.strip()) == 0:\n",
    "        return None\n",
    "    \n",
    "    response_clean = response.strip()\n",
    "    \n",
    "    # 1. 直接匹配纯数字\n",
    "    if response_clean == '1':\n",
    "        return 1\n",
    "    if response_clean == '-1':\n",
    "        return -1\n",
    "    \n",
    "    # 2. 使用正则表达式提取数字\n",
    "    numbers = re.findall(r'-?\\d+', response_clean)\n",
    "    for num in numbers:\n",
    "        if num == '-1':\n",
    "            return -1\n",
    "        elif num == '1':\n",
    "            return 1\n",
    "    \n",
    "    # 3. 关键词匹配（英文）\n",
    "    response_lower = response_clean.lower()\n",
    "    \n",
    "    positive_keywords = [\n",
    "        'positive', 'good', 'great', 'excellent', 'wonderful', 'amazing',\n",
    "        'love', 'like', 'enjoy', 'brilliant', 'fantastic', 'awesome'\n",
    "    ]\n",
    "    \n",
    "    negative_keywords = [\n",
    "        'negative', 'bad', 'poor', 'terrible', 'awful', 'horrible',\n",
    "        'hate', 'dislike', 'boring', 'disappointing', 'failure'\n",
    "    ]\n",
    "    \n",
    "    pos_count = sum(1 for word in positive_keywords if word in response_lower)\n",
    "    neg_count = sum(1 for word in negative_keywords if word in response_lower)\n",
    "    \n",
    "    if pos_count > neg_count:\n",
    "        return 1\n",
    "    elif neg_count > pos_count:\n",
    "        return -1\n",
    "    \n",
    "    # 4. 检查是否包含情感表述\n",
    "    if 'positive' in response_lower:\n",
    "        return 1\n",
    "    elif 'negative' in response_lower:\n",
    "        return -1\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 主循环\n",
    "results = []\n",
    "print(\"开始情感分析...\")\n",
    "\n",
    "for i, item in enumerate(dataset):\n",
    "    text = item['sentence']\n",
    "    true_label = item['label']\n",
    "    \n",
    "    response = query_ollama(improved_prompt.format(text))\n",
    "    predicted_sentiment = robust_sentiment_parser(response)\n",
    "    true_sentiment = 1 if true_label == 1 else -1\n",
    "    \n",
    "    is_correct = predicted_sentiment == true_sentiment if predicted_sentiment is not None else False\n",
    "    \n",
    "    results.append({\n",
    "        'text': text,\n",
    "        'true_sentiment': true_sentiment,\n",
    "        'predicted_sentiment': predicted_sentiment,\n",
    "        'response': response,\n",
    "        'correct': is_correct\n",
    "    })\n",
    "    \n",
    "    # 只显示有问题的样本或每10个显示一次\n",
    "    # if predicted_sentiment is None or not is_correct or i % 10 == 0:\n",
    "    #     print(f\"样本 {i+1}:\")\n",
    "    #     print(f\"文本: {text}\")\n",
    "    #     print(f\"回复: {response}\")\n",
    "    #     print(f\"真实: {true_sentiment}, 预测: {predicted_sentiment}, 正确: {is_correct}\")\n",
    "    #     print(\"-\" * 80)\n",
    "    \n",
    "    # time.sleep(1)\n",
    "\n",
    "# 统计结果\n",
    "valid_predictions = [r for r in results if r['predicted_sentiment'] is not None]\n",
    "correct_predictions = sum(1 for r in valid_predictions if r['correct'])\n",
    "total_valid = len(valid_predictions)\n",
    "\n",
    "print(f\"\\n结果统计:\")\n",
    "print(f\"总样本数: {len(results)}\")\n",
    "print(f\"有效预测数: {total_valid}\")\n",
    "print(f\"有效预测准确率: {correct_predictions/total_valid:.2%}\" if total_valid > 0 else \"无有效预测\")\n",
    "print(f\"无效预测数: {len(results) - total_valid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f623f64-f204-47c8-a6bc-f59bc070d121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始情感分析...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32mD:\\anacoda\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m sock\u001b[38;5;241m.\u001b[39mconnect(sa)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] 由于目标计算机积极拒绝，无法连接。",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 98\u001b[0m\n\u001b[0;32m     95\u001b[0m text \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     96\u001b[0m true_label \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 98\u001b[0m response \u001b[38;5;241m=\u001b[39m query_ollama(improved_prompt\u001b[38;5;241m.\u001b[39mformat(text))\n\u001b[0;32m     99\u001b[0m predicted_sentiment \u001b[38;5;241m=\u001b[39m robust_sentiment_parser(response)\n\u001b[0;32m    100\u001b[0m true_sentiment \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m true_label \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[32], line 23\u001b[0m, in \u001b[0;36mquery_ollama\u001b[1;34m(prompt, model)\u001b[0m\n\u001b[0;32m     12\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     }\n\u001b[0;32m     20\u001b[0m }\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 23\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, json\u001b[38;5;241m=\u001b[39mdata, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mD:\\anacoda\\Lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m request(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url, data\u001b[38;5;241m=\u001b[39mdata, json\u001b[38;5;241m=\u001b[39mjson, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anacoda\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m session\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anacoda\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mD:\\anacoda\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mD:\\anacoda\\Lib\\site-packages\\requests\\adapters.py:644\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    641\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m     resp \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(\n\u001b[0;32m    645\u001b[0m         method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    646\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    647\u001b[0m         body\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbody,\n\u001b[0;32m    648\u001b[0m         headers\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    649\u001b[0m         redirect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    650\u001b[0m         assert_same_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    651\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    652\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    653\u001b[0m         retries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_retries,\n\u001b[0;32m    654\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    655\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    656\u001b[0m     )\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mD:\\anacoda\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    784\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    788\u001b[0m     conn,\n\u001b[0;32m    789\u001b[0m     method,\n\u001b[0;32m    790\u001b[0m     url,\n\u001b[0;32m    791\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout_obj,\n\u001b[0;32m    792\u001b[0m     body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    793\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    794\u001b[0m     chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    795\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m    796\u001b[0m     response_conn\u001b[38;5;241m=\u001b[39mresponse_conn,\n\u001b[0;32m    797\u001b[0m     preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    798\u001b[0m     decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kw,\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    803\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anacoda\\Lib\\site-packages\\urllib3\\connectionpool.py:493\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 493\u001b[0m     conn\u001b[38;5;241m.\u001b[39mrequest(\n\u001b[0;32m    494\u001b[0m         method,\n\u001b[0;32m    495\u001b[0m         url,\n\u001b[0;32m    496\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[0;32m    497\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m    498\u001b[0m         chunked\u001b[38;5;241m=\u001b[39mchunked,\n\u001b[0;32m    499\u001b[0m         preload_content\u001b[38;5;241m=\u001b[39mpreload_content,\n\u001b[0;32m    500\u001b[0m         decode_content\u001b[38;5;241m=\u001b[39mdecode_content,\n\u001b[0;32m    501\u001b[0m         enforce_content_length\u001b[38;5;241m=\u001b[39menforce_content_length,\n\u001b[0;32m    502\u001b[0m     )\n\u001b[0;32m    504\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBrokenPipeError\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\anacoda\\Lib\\site-packages\\urllib3\\connection.py:494\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m header, value \u001b[38;5;129;01min\u001b[39;00m headers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[1;32m--> 494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendheaders()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\anacoda\\Lib\\http\\client.py:1333\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_output(message_body, encode_chunked\u001b[38;5;241m=\u001b[39mencode_chunked)\n",
      "File \u001b[1;32mD:\\anacoda\\Lib\\http\\client.py:1093\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1091\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer)\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1093\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(msg)\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1096\u001b[0m \n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1099\u001b[0m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anacoda\\Lib\\http\\client.py:1037\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m-> 1037\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect()\n\u001b[0;32m   1038\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1039\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "File \u001b[1;32mD:\\anacoda\\Lib\\site-packages\\urllib3\\connection.py:325\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_conn()\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_connected_to_proxy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anacoda\\Lib\\site-packages\\urllib3\\connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \n\u001b[0;32m    195\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    199\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport),\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout,\n\u001b[0;32m    201\u001b[0m         source_address\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_address,\n\u001b[0;32m    202\u001b[0m         socket_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket_options,\n\u001b[0;32m    203\u001b[0m     )\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anacoda\\Lib\\site-packages\\urllib3\\util\\connection.py:81\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     79\u001b[0m         err \u001b[38;5;241m=\u001b[39m _\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m             sock\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\anacoda\\Lib\\socket.py:501\u001b[0m, in \u001b[0;36msocket.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_real_close\u001b[39m(\u001b[38;5;28mself\u001b[39m, _ss\u001b[38;5;241m=\u001b[39m_socket\u001b[38;5;241m.\u001b[39msocket):\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;66;03m# This function should not reference any globals. See issue #808164.\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     _ss\u001b[38;5;241m.\u001b[39mclose(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;66;03m# This function should not reference any globals. See issue #808164.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_io_refs \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"test[:20]\")\n",
    "\n",
    "def query_ollama(prompt, model=\"deepseek-coder:6.7b\"):\n",
    "    \"\"\"调用Ollama模型\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.1,  # 降低随机性\n",
    "            \"num_predict\": 50    # 限制输出长度\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=data, timeout=60)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"response\"]\n",
    "        else:\n",
    "            return f\"错误: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"请求失败: {str(e)}\"\n",
    "\n",
    "# 改进的提示词\n",
    "improved_prompt = \"\"\"判断以下电影评论的情感，只返回一个数字：\n",
    "- 正面情感返回 1\n",
    "- 负面情感返回 -1\n",
    "\n",
    "评论：\"{0}\"\n",
    "\n",
    "情感：\"\"\"\n",
    "\n",
    "def robust_sentiment_parser(response):\n",
    "    \"\"\"鲁棒的情感解析器\"\"\"\n",
    "    if not response or len(response.strip()) == 0:\n",
    "        return None\n",
    "    \n",
    "    response_clean = response.strip()\n",
    "    \n",
    "    # 1. 直接匹配纯数字\n",
    "    if response_clean == '1':\n",
    "        return 1\n",
    "    if response_clean == '-1':\n",
    "        return -1\n",
    "    \n",
    "    # 2. 使用正则表达式提取数字\n",
    "    numbers = re.findall(r'-?\\d+', response_clean)\n",
    "    for num in numbers:\n",
    "        if num == '-1':\n",
    "            return -1\n",
    "        elif num == '1':\n",
    "            return 1\n",
    "    \n",
    "    # 3. 关键词匹配（英文）\n",
    "    response_lower = response_clean.lower()\n",
    "    \n",
    "    positive_keywords = [\n",
    "        'positive', 'good', 'great', 'excellent', 'wonderful', 'amazing',\n",
    "        'love', 'like', 'enjoy', 'brilliant', 'fantastic', 'awesome'\n",
    "    ]\n",
    "    \n",
    "    negative_keywords = [\n",
    "        'negative', 'bad', 'poor', 'terrible', 'awful', 'horrible',\n",
    "        'hate', 'dislike', 'boring', 'disappointing', 'failure'\n",
    "    ]\n",
    "    \n",
    "    pos_count = sum(1 for word in positive_keywords if word in response_lower)\n",
    "    neg_count = sum(1 for word in negative_keywords if word in response_lower)\n",
    "    \n",
    "    if pos_count > neg_count:\n",
    "        return 1\n",
    "    elif neg_count > pos_count:\n",
    "        return -1\n",
    "    \n",
    "    # 4. 检查是否包含情感表述\n",
    "    if 'positive' in response_lower:\n",
    "        return 1\n",
    "    elif 'negative' in response_lower:\n",
    "        return -1\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 主循环\n",
    "results = []\n",
    "print(\"开始情感分析...\")\n",
    "\n",
    "for i, item in enumerate(dataset):\n",
    "    text = item['sentence']\n",
    "    true_label = item['label']\n",
    "    \n",
    "    response = query_ollama(improved_prompt.format(text))\n",
    "    predicted_sentiment = robust_sentiment_parser(response)\n",
    "    true_sentiment = 1 if true_label == 1 else -1\n",
    "    \n",
    "    is_correct = predicted_sentiment == true_sentiment if predicted_sentiment is not None else False\n",
    "    \n",
    "    results.append({\n",
    "        'text': text,\n",
    "        'true_sentiment': true_sentiment,\n",
    "        'predicted_sentiment': predicted_sentiment,\n",
    "        'response': response,\n",
    "        'correct': is_correct\n",
    "    })\n",
    "    \n",
    "    # 只显示有问题的样本或每10个显示一次\n",
    "    # if predicted_sentiment is None or not is_correct or i % 10 == 0:\n",
    "    #     print(f\"样本 {i+1}:\")\n",
    "    #     print(f\"文本: {text}\")\n",
    "    #     print(f\"回复: {response}\")\n",
    "    #     print(f\"真实: {true_sentiment}, 预测: {predicted_sentiment}, 正确: {is_correct}\")\n",
    "    #     print(\"-\" * 80)\n",
    "    \n",
    "    # time.sleep(1)\n",
    "\n",
    "# 统计结果\n",
    "valid_predictions = [r for r in results if r['predicted_sentiment'] is not None]\n",
    "correct_predictions = sum(1 for r in valid_predictions if r['correct'])\n",
    "total_valid = len(valid_predictions)\n",
    "\n",
    "#print(f\"\\n结果统计:\")\n",
    "#print(f\"总样本数: {len(results)}\")\n",
    "print(f\"有效预测数: {total_valid}\")\n",
    "print(f\"有效预测准确率: {correct_predictions/total_valid:.2%}\" if total_valid > 0 else \"无有效预测\")\n",
    "print(f\"无效预测数: {len(results) - total_valid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00f40407-c349-4a6d-b2a5-868fa862b12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始情感分析...\n",
      "样本 1:\n",
      "文本: uneasy mishmash of styles and genres .\n",
      "回复: -1\n",
      "真实: -1, 预测: -1, 正确: True\n",
      "--------------------------------------------------------------------------------\n",
      "样本 3:\n",
      "文本: by the end of no such thing the audience , like beatrice , has a watchful affection for the monster .\n",
      "回复: 1\n",
      "真实: -1, 预测: 1, 正确: False\n",
      "--------------------------------------------------------------------------------\n",
      "样本 4:\n",
      "文本: director rob marshall went out gunning to make a great one .\n",
      "回复: 1\n",
      "真实: -1, 预测: 1, 正确: False\n",
      "--------------------------------------------------------------------------------\n",
      "样本 5:\n",
      "文本: lathan and diggs have considerable personal charm , and their screen rapport makes the old story seem new .\n",
      "回复: 1\n",
      "真实: -1, 预测: 1, 正确: False\n",
      "--------------------------------------------------------------------------------\n",
      "样本 6:\n",
      "文本: a well-made and often lovely depiction of the mysteries of friendship .\n",
      "回复: 1\n",
      "真实: -1, 预测: 1, 正确: False\n",
      "--------------------------------------------------------------------------------\n",
      "样本 7:\n",
      "文本: none of this violates the letter of behan 's book , but missing is its spirit , its ribald , full-throated humor .\n",
      "回复: 1\n",
      "真实: -1, 预测: 1, 正确: False\n",
      "--------------------------------------------------------------------------------\n",
      "样本 8:\n",
      "文本: although it bangs a very cliched drum at times , this crowd-pleaser 's fresh dialogue , energetic music , and good-natured spunk are often infectious .\n",
      "回复: 1\n",
      "真实: -1, 预测: 1, 正确: False\n",
      "--------------------------------------------------------------------------------\n",
      "样本 9:\n",
      "文本: it is not a mass-market entertainment but an uncompromising attempt by one artist to think about another .\n",
      "回复: 1\n",
      "真实: -1, 预测: 1, 正确: False\n",
      "--------------------------------------------------------------------------------\n",
      "样本 11:\n",
      "文本: it 's also heavy-handed and devotes too much time to bigoted views .\n",
      "回复: -1\n",
      "真实: -1, 预测: -1, 正确: True\n",
      "--------------------------------------------------------------------------------\n",
      "样本 12:\n",
      "文本: it helps that lil bow wow ... tones down his pint-sized gangsta act to play someone who resembles a real kid .\n",
      "回复: 1\n",
      "真实: -1, 预测: 1, 正确: False\n",
      "--------------------------------------------------------------------------------\n",
      "样本 13:\n",
      "文本: watching the film is like reading a times portrait of grief that keeps shifting focus to the journalist who wrote it .\n",
      "回复: 1\n",
      "真实: -1, 预测: 1, 正确: False\n",
      "--------------------------------------------------------------------------------\n",
      "样本 14:\n",
      "文本: moore 's performance impresses almost as much as her work with haynes in 1995 's safe .\n",
      "回复: 1\n",
      "真实: -1, 预测: 1, 正确: False\n",
      "--------------------------------------------------------------------------------\n",
      "样本 15:\n",
      "文本: reinforces the talents of screenwriter charlie kaufman , creator of adaptation and being john malkovich .\n",
      "回复: 1\n",
      "真实: -1, 预测: 1, 正确: False\n",
      "--------------------------------------------------------------------------------\n",
      "样本 16:\n",
      "文本: now trimmed by about 20 minutes , this lavish three-year-old production has enough grandeur and scale to satisfy as grown-up escapism .\n",
      "回复: 1\n",
      "真实: -1, 预测: 1, 正确: False\n",
      "--------------------------------------------------------------------------------\n",
      "样本 17:\n",
      "文本: a journey through memory , a celebration of living , and a sobering rumination on fatality , classism , and ignorance .\n",
      "回复: 1\n",
      "真实: -1, 预测: 1, 正确: False\n",
      "--------------------------------------------------------------------------------\n",
      "样本 18:\n",
      "文本: a remarkable 179-minute meditation on the nature of revolution .\n",
      "回复: 1\n",
      "真实: -1, 预测: 1, 正确: False\n",
      "--------------------------------------------------------------------------------\n",
      "样本 19:\n",
      "文本: waydowntown is by no means a perfect film , but its boasts a huge charm factor and smacks of originality .\n",
      "回复: 1\n",
      "真实: -1, 预测: 1, 正确: False\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "结果统计:\n",
      "总样本数: 20\n",
      "有效预测数: 20\n",
      "有效预测准确率: 25.00%\n",
      "无效预测数: 0\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"test[:20]\")\n",
    "\n",
    "def query_ollama(prompt, model=\"gemma:2b-instruct\"):\n",
    "    \"\"\"调用Ollama模型\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.1,  # 降低随机性\n",
    "            \"num_predict\": 50    # 限制输出长度\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=data, timeout=60)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"response\"]\n",
    "        else:\n",
    "            return f\"错误: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"请求失败: {str(e)}\"\n",
    "\n",
    "# 改进的提示词\n",
    "improved_prompt = \"\"\"判断以下电影评论的情感，只返回一个数字：\n",
    "- 正面情感返回 1\n",
    "- 负面情感返回 -1\n",
    "\n",
    "评论：\"{0}\"\n",
    "\n",
    "情感：\"\"\"\n",
    "\n",
    "def robust_sentiment_parser(response):\n",
    "    \"\"\"鲁棒的情感解析器\"\"\"\n",
    "    if not response or len(response.strip()) == 0:\n",
    "        return None\n",
    "    \n",
    "    response_clean = response.strip()\n",
    "    \n",
    "    # 1. 直接匹配纯数字\n",
    "    if response_clean == '1':\n",
    "        return 1\n",
    "    if response_clean == '-1':\n",
    "        return -1\n",
    "    \n",
    "    # 2. 使用正则表达式提取数字\n",
    "    numbers = re.findall(r'-?\\d+', response_clean)\n",
    "    for num in numbers:\n",
    "        if num == '-1':\n",
    "            return -1\n",
    "        elif num == '1':\n",
    "            return 1\n",
    "    \n",
    "    # 3. 关键词匹配（英文）\n",
    "    response_lower = response_clean.lower()\n",
    "    \n",
    "    positive_keywords = [\n",
    "        'positive', 'good', 'great', 'excellent', 'wonderful', 'amazing',\n",
    "        'love', 'like', 'enjoy', 'brilliant', 'fantastic', 'awesome'\n",
    "    ]\n",
    "    \n",
    "    negative_keywords = [\n",
    "        'negative', 'bad', 'poor', 'terrible', 'awful', 'horrible',\n",
    "        'hate', 'dislike', 'boring', 'disappointing', 'failure'\n",
    "    ]\n",
    "    \n",
    "    pos_count = sum(1 for word in positive_keywords if word in response_lower)\n",
    "    neg_count = sum(1 for word in negative_keywords if word in response_lower)\n",
    "    \n",
    "    if pos_count > neg_count:\n",
    "        return 1\n",
    "    elif neg_count > pos_count:\n",
    "        return -1\n",
    "    \n",
    "    # 4. 检查是否包含情感表述\n",
    "    if 'positive' in response_lower:\n",
    "        return 1\n",
    "    elif 'negative' in response_lower:\n",
    "        return -1\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 主循环\n",
    "results = []\n",
    "print(\"开始情感分析...\")\n",
    "\n",
    "for i, item in enumerate(dataset):\n",
    "    text = item['sentence']\n",
    "    true_label = item['label']\n",
    "    \n",
    "    response = query_ollama(improved_prompt.format(text))\n",
    "    predicted_sentiment = robust_sentiment_parser(response)\n",
    "    true_sentiment = 1 if true_label == 1 else -1\n",
    "    \n",
    "    is_correct = predicted_sentiment == true_sentiment if predicted_sentiment is not None else False\n",
    "    \n",
    "    results.append({\n",
    "        'text': text,\n",
    "        'true_sentiment': true_sentiment,\n",
    "        'predicted_sentiment': predicted_sentiment,\n",
    "        'response': response,\n",
    "        'correct': is_correct\n",
    "    })\n",
    "    \n",
    "    # 只显示有问题的样本或每10个显示一次\n",
    "    if predicted_sentiment is None or not is_correct or i % 10 == 0:\n",
    "        print(f\"样本 {i+1}:\")\n",
    "        print(f\"文本: {text}\")\n",
    "        print(f\"回复: {response}\")\n",
    "        print(f\"真实: {true_sentiment}, 预测: {predicted_sentiment}, 正确: {is_correct}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "# 统计结果\n",
    "valid_predictions = [r for r in results if r['predicted_sentiment'] is not None]\n",
    "correct_predictions = sum(1 for r in valid_predictions if r['correct'])\n",
    "total_valid = len(valid_predictions)\n",
    "\n",
    "print(f\"\\n结果统计:\")\n",
    "print(f\"总样本数: {len(results)}\")\n",
    "print(f\"有效预测数: {total_valid}\")\n",
    "print(f\"有效预测准确率: {correct_predictions/total_valid:.2%}\" if total_valid > 0 else \"无有效预测\")\n",
    "print(f\"无效预测数: {len(results) - total_valid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7a79e51-2aeb-42ac-bfd3-5ddd400ce6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始情感分析...\n",
      "\n",
      "结果统计:\n",
      "总样本数: 20\n",
      "有效预测数: 19\n",
      "有效预测准确率: 26.32%\n",
      "无效预测数: 1\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"test[:20]\")\n",
    "\n",
    "def query_ollama(prompt, model=\"phi:2.7b\"):\n",
    "    \"\"\"调用Ollama模型\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.1,  # 降低随机性\n",
    "            \"num_predict\": 50    # 限制输出长度\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=data, timeout=60)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"response\"]\n",
    "        else:\n",
    "            return f\"错误: {response.status_code}\"\n",
    "    except Exception as e:\n",
    "        return f\"请求失败: {str(e)}\"\n",
    "\n",
    "# 改进的提示词\n",
    "improved_prompt = \"\"\"判断以下电影评论的情感，只返回一个数字：\n",
    "- 正面情感返回 1\n",
    "- 负面情感返回 -1\n",
    "\n",
    "评论：\"{0}\"\n",
    "\n",
    "情感：\"\"\"\n",
    "\n",
    "def robust_sentiment_parser(response):\n",
    "    \"\"\"鲁棒的情感解析器\"\"\"\n",
    "    if not response or len(response.strip()) == 0:\n",
    "        return None\n",
    "    \n",
    "    response_clean = response.strip()\n",
    "    \n",
    "    # 1. 直接匹配纯数字\n",
    "    if response_clean == '1':\n",
    "        return 1\n",
    "    if response_clean == '-1':\n",
    "        return -1\n",
    "    \n",
    "    # 2. 使用正则表达式提取数字\n",
    "    numbers = re.findall(r'-?\\d+', response_clean)\n",
    "    for num in numbers:\n",
    "        if num == '-1':\n",
    "            return -1\n",
    "        elif num == '1':\n",
    "            return 1\n",
    "    \n",
    "    # 3. 关键词匹配（英文）\n",
    "    response_lower = response_clean.lower()\n",
    "    \n",
    "    positive_keywords = [\n",
    "        'positive', 'good', 'great', 'excellent', 'wonderful', 'amazing',\n",
    "        'love', 'like', 'enjoy', 'brilliant', 'fantastic', 'awesome'\n",
    "    ]\n",
    "    \n",
    "    negative_keywords = [\n",
    "        'negative', 'bad', 'poor', 'terrible', 'awful', 'horrible',\n",
    "        'hate', 'dislike', 'boring', 'disappointing', 'failure'\n",
    "    ]\n",
    "    \n",
    "    pos_count = sum(1 for word in positive_keywords if word in response_lower)\n",
    "    neg_count = sum(1 for word in negative_keywords if word in response_lower)\n",
    "    \n",
    "    if pos_count > neg_count:\n",
    "        return 1\n",
    "    elif neg_count > pos_count:\n",
    "        return -1\n",
    "    \n",
    "    # 4. 检查是否包含情感表述\n",
    "    if 'positive' in response_lower:\n",
    "        return 1\n",
    "    elif 'negative' in response_lower:\n",
    "        return -1\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 主循环\n",
    "results = []\n",
    "print(\"开始情感分析...\")\n",
    "\n",
    "for i, item in enumerate(dataset):\n",
    "    text = item['sentence']\n",
    "    true_label = item['label']\n",
    "    \n",
    "    response = query_ollama(improved_prompt.format(text))\n",
    "    predicted_sentiment = robust_sentiment_parser(response)\n",
    "    true_sentiment = 1 if true_label == 1 else -1\n",
    "    \n",
    "    is_correct = predicted_sentiment == true_sentiment if predicted_sentiment is not None else False\n",
    "    \n",
    "    results.append({\n",
    "        'text': text,\n",
    "        'true_sentiment': true_sentiment,\n",
    "        'predicted_sentiment': predicted_sentiment,\n",
    "        'response': response,\n",
    "        'correct': is_correct\n",
    "    })\n",
    "    \n",
    "    # 只显示有问题的样本或每10个显示一次\n",
    "    # if predicted_sentiment is None or not is_correct or i % 10 == 0:\n",
    "    #     print(f\"样本 {i+1}:\")\n",
    "    #     print(f\"文本: {text}\")\n",
    "    #     print(f\"回复: {response}\")\n",
    "    #     print(f\"真实: {true_sentiment}, 预测: {predicted_sentiment}, 正确: {is_correct}\")\n",
    "    #     print(\"-\" * 80)\n",
    "    \n",
    "    # time.sleep(1)\n",
    "\n",
    "# 统计结果\n",
    "valid_predictions = [r for r in results if r['predicted_sentiment'] is not None]\n",
    "correct_predictions = sum(1 for r in valid_predictions if r['correct'])\n",
    "total_valid = len(valid_predictions)\n",
    "\n",
    "print(f\"\\n结果统计:\")\n",
    "print(f\"总样本数: {len(results)}\")\n",
    "print(f\"有效预测数: {total_valid}\")\n",
    "print(f\"有效预测准确率: {correct_predictions/total_valid:.2%}\" if total_valid > 0 else \"无有效预测\")\n",
    "print(f\"无效预测数: {len(results) - total_valid}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4128e41-e96e-4d3b-b9fe-ae6d6b767b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始情感分析...\n",
      "Ollama服务连接成功\n",
      "处理样本 1/20: uneasy mishmash of styles and genres ....\n",
      "处理样本 2/20: this film 's relationship to actual tension is the...\n",
      "处理样本 3/20: by the end of no such thing the audience , like be...\n",
      "处理样本 4/20: director rob marshall went out gunning to make a g...\n",
      "处理样本 5/20: lathan and diggs have considerable personal charm ...\n",
      "处理样本 6/20: a well-made and often lovely depiction of the myst...\n",
      "处理样本 7/20: none of this violates the letter of behan 's book ...\n",
      "处理样本 8/20: although it bangs a very cliched drum at times , t...\n",
      "处理样本 9/20: it is not a mass-market entertainment but an uncom...\n",
      "处理样本 10/20: this is junk food cinema at its greasiest ....\n",
      "处理样本 11/20: it 's also heavy-handed and devotes too much time ...\n",
      "处理样本 12/20: it helps that lil bow wow ... tones down his pint-...\n",
      "处理样本 13/20: watching the film is like reading a times portrait...\n",
      "处理样本 14/20: moore 's performance impresses almost as much as h...\n",
      "处理样本 15/20: reinforces the talents of screenwriter charlie kau...\n",
      "处理样本 16/20: now trimmed by about 20 minutes , this lavish thre...\n",
      "处理样本 17/20: a journey through memory , a celebration of living...\n",
      "处理样本 18/20: a remarkable 179-minute meditation on the nature o...\n",
      "处理样本 19/20: waydowntown is by no means a perfect film , but it...\n",
      "处理样本 20/20: it 's just incredibly dull ....\n",
      "\n",
      "结果统计:\n",
      "总样本数: 20\n",
      "成功请求数: 20\n",
      "有效预测数: 20\n",
      "有效预测准确率: 45.00%\n",
      "无效预测数: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 设置使用CPU\n",
    "os.environ['OLLAMA_CPU'] = '1'\n",
    "from datasets import load_dataset\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 加载数据集\n",
    "dataset = load_dataset(\"glue\", \"sst2\", split=\"test[:20]\")\n",
    "\n",
    "def query_ollama(prompt, model=\"mistral\"):\n",
    "    \"\"\"调用Ollama模型\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.1,  # 降低随机性\n",
    "            \"num_predict\": 50    # 限制输出长度\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=data, timeout=120)  # 增加超时时间\n",
    "        if response.status_code == 200:\n",
    "            return response.json()[\"response\"]\n",
    "        else:\n",
    "            print(f\"API错误: {response.status_code} - {response.text}\")\n",
    "            return f\"错误: {response.status_code}\"\n",
    "    except requests.exceptions.Timeout:\n",
    "        return \"错误: 请求超时\"\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return \"错误: 无法连接到Ollama服务，请确保Ollama正在运行\"\n",
    "    except Exception as e:\n",
    "        return f\"请求失败: {str(e)}\"\n",
    "\n",
    "# 改进的提示词\n",
    "improved_prompt = \"\"\"请分析以下电影评论的情感倾向，只返回一个数字：\n",
    "- 如果是正面情感，返回 1\n",
    "- 如果是负面情感，返回 -1\n",
    "\n",
    "评论：\"{0}\"\n",
    "\n",
    "请只返回数字，不要有其他文字：\"\"\"\n",
    "\n",
    "def robust_sentiment_parser(response):\n",
    "    \"\"\"鲁棒的情感解析器\"\"\"\n",
    "    if not response or len(response.strip()) == 0:\n",
    "        return None\n",
    "    \n",
    "    response_clean = response.strip()\n",
    "    \n",
    "    # 1. 直接匹配纯数字\n",
    "    if response_clean == '1':\n",
    "        return 1\n",
    "    if response_clean == '-1':\n",
    "        return -1\n",
    "    \n",
    "    # 2. 使用正则表达式提取数字\n",
    "    numbers = re.findall(r'-?\\d+', response_clean)\n",
    "    for num in numbers:\n",
    "        if num == '-1':\n",
    "            return -1\n",
    "        elif num == '1':\n",
    "            return 1\n",
    "    \n",
    "    # 3. 关键词匹配（英文）\n",
    "    response_lower = response_clean.lower()\n",
    "    \n",
    "    positive_keywords = [\n",
    "        'positive', 'good', 'great', 'excellent', 'wonderful', 'amazing',\n",
    "        'love', 'like', 'enjoy', 'brilliant', 'fantastic', 'awesome', 'positive'\n",
    "    ]\n",
    "    \n",
    "    negative_keywords = [\n",
    "        'negative', 'bad', 'poor', 'terrible', 'awful', 'horrible',\n",
    "        'hate', 'dislike', 'boring', 'disappointing', 'failure', 'negative'\n",
    "    ]\n",
    "    \n",
    "    pos_count = sum(1 for word in positive_keywords if word in response_lower)\n",
    "    neg_count = sum(1 for word in negative_keywords if word in response_lower)\n",
    "    \n",
    "    if pos_count > neg_count:\n",
    "        return 1\n",
    "    elif neg_count > pos_count:\n",
    "        return -1\n",
    "    \n",
    "    # 4. 检查是否包含情感表述\n",
    "    if 'positive' in response_lower:\n",
    "        return 1\n",
    "    elif 'negative' in response_lower:\n",
    "        return -1\n",
    "    \n",
    "    return None\n",
    "\n",
    "# 检查Ollama服务是否可用\n",
    "def check_ollama_availability():\n",
    "    \"\"\"检查Ollama服务是否可用\"\"\"\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=10)\n",
    "        return response.status_code == 200\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# 主循环\n",
    "print(\"开始情感分析...\")\n",
    "\n",
    "# 检查服务\n",
    "if not check_ollama_availability():\n",
    "    print(\"错误: 无法连接到Ollama服务，请确保Ollama正在运行\")\n",
    "    print(\"请在命令行中运行: ollama serve\")\n",
    "else:\n",
    "    print(\"Ollama服务连接成功\")\n",
    "    \n",
    "    results = []\n",
    "    successful_requests = 0\n",
    "    \n",
    "    for i, item in enumerate(dataset):\n",
    "        text = item['sentence']\n",
    "        true_label = item['label']\n",
    "        \n",
    "        # print(f\"处理样本 {i+1}/{len(dataset)}: {text[:50]}...\")\n",
    "        \n",
    "        response = query_ollama(improved_prompt.format(text))\n",
    "        predicted_sentiment = robust_sentiment_parser(response)\n",
    "        true_sentiment = 1 if true_label == 1 else -1\n",
    "        \n",
    "        is_correct = predicted_sentiment == true_sentiment if predicted_sentiment is not None else False\n",
    "        \n",
    "        results.append({\n",
    "            'text': text,\n",
    "            'true_sentiment': true_sentiment,\n",
    "            'predicted_sentiment': predicted_sentiment,\n",
    "            'response': response,\n",
    "            'correct': is_correct\n",
    "        })\n",
    "        \n",
    "        # 显示详细结果\n",
    "        # print(f\"样本 {i+1}:\")\n",
    "        # print(f\"文本: {text}\")\n",
    "        # print(f\"回复: {response}\")\n",
    "        # print(f\"真实: {true_sentiment}, 预测: {predicted_sentiment}, 正确: {is_correct}\")\n",
    "        # print(\"-\" * 80)\n",
    "        \n",
    "        # 统计成功请求\n",
    "        if not response.startswith(\"错误\"):\n",
    "            successful_requests += 1\n",
    "        \n",
    "        # 增加延迟以避免过载\n",
    "        time.sleep(2)\n",
    "\n",
    "    # 统计结果\n",
    "    valid_predictions = [r for r in results if r['predicted_sentiment'] is not None]\n",
    "    correct_predictions = sum(1 for r in valid_predictions if r['correct'])\n",
    "    total_valid = len(valid_predictions)\n",
    "    \n",
    "    print(f\"\\n结果统计:\")\n",
    "    print(f\"总样本数: {len(results)}\")\n",
    "    print(f\"成功请求数: {successful_requests}\")\n",
    "    print(f\"有效预测数: {total_valid}\")\n",
    "    if total_valid > 0:\n",
    "        print(f\"有效预测准确率: {correct_predictions/total_valid:.2%}\")\n",
    "    else:\n",
    "        print(\"无有效预测\")\n",
    "    print(f\"无效预测数: {len(results) - total_valid}\")\n",
    "    \n",
    "    # 显示错误详情\n",
    "    error_samples = [r for r in results if r['response'].startswith(\"错误\")]\n",
    "    if error_samples:\n",
    "        print(f\"\\n错误样本 ({len(error_samples)}个):\")\n",
    "        for i, sample in enumerate(error_samples[:5]):  # 只显示前5个错误\n",
    "            print(f\"  {i+1}. {sample['text'][:50]}... -> {sample['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad91d92-1b9f-4b00-9735-efd71292fbca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
